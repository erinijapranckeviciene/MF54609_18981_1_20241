{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41375591-b8c2-4b9f-997b-2156eb71c126",
   "metadata": {},
   "source": [
    "# FC book Ch. 11 Natural language processing. IMDB reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779907c-2969-4cde-aaac-8cb9bd78e295",
   "metadata": {},
   "source": [
    "## Prepare IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252a69bf-fc61-496e-87e2-12bee674689e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDATA ALREADY PREPARED\\n\\nbase_dir = pathlib.Path(\"aclImdb\")\\nval_dir = base_dir / \"val\"\\ntrain_dir = base_dir / \"train\"\\nfor category in (\"neg\", \"pos\"):\\n    os.makedirs(val_dir / category)\\n    files = os.listdir(train_dir / category)\\n    random.Random(1337).shuffle(files)\\n    num_val_samples = int(0.2 * len(files))\\n    val_files = files[-num_val_samples:]\\n    for fname in val_files:\\n        shutil.move(train_dir / category / fname, val_dir / category / fname)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "\"\"\"\n",
    "DATA ALREADY PREPARED\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a7ee70-e2cc-43d9-bbcf-fb4147f3c0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 21:54:27.499605: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-14 21:54:27.499648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-14 21:54:27.500841: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-14 21:54:27.507146: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-14 21:54:28.163319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 21:54:29.407539: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-14 21:54:29.445835: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-14 21:54:29.446247: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-14 21:54:29.447552: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-14 21:54:29.447978: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-14 21:54:29.448282: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-14 21:54:29.547439: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-14 21:54:29.547651: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-14 21:54:29.547832: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-14 21:54:29.547981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4785 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-01-14 21:54:29.724564: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size )\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a74a99-e19c-40a0-8ca0-1e71711319b5",
   "metadata": {},
   "source": [
    "## Listing 11.2\n",
    "Displaying the shapes and dtypes of the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87d94172-d37b-45d0-b9be-81f1d488fa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'Why a stupid, boring, crappy overrated film series like \"Star Wars\" gets all the hype, and a truly amazing film like this one goes completely un-noticed.. is beyond me... This movie will really open your eyes to the dark, disturbing, sad, and scary world we live in...<br /><br />Unlike the boring \"Elephant\", this movie isn\\'t one of those \"just a typical day until someome pulls the trigger\" movies.. this movie focuses more on what happens AFTER the event...<br /><br />Deana, played by the very hot and very talented Erika Christensen, is a happy and healthy straight-A student with great friends and a great life... until... she is injured on the day of the shooting, by being shot in the head.. Luckily she is not killed, but is severely injured and has to be in the hospital for a while, causing her to be in a lot of emotional pain, in addition to the physical...<br /><br />Meanwhile, Alicia, played by the also very gorgeous and talented Busy Phillips, is a nasty, cold-hearted, rebellious, anti-social goth girl who doesn\\'t have a single positive trait on her... and she is unharmed when the shooting happens.. because it turns out, she was FRIENDS with the shooter and knew he was going to do what he did... which causes her to be brought into the police station and be asked some questions.. When she refuses to tell the cops if she knew the shooting was going to happen, they constantly come by her house to try to convince her to say something... and she still doesn\\'t, so the principal of the school makes her attend a funeral of one of the dead students, and after she walks out on that... the principal decides enough is enough, and forces her to go visit Deana in the hospital.. Of course she refuses this too, but the principal says that if Alicia doesn\\'t do this, the cops are going to continue to try to get her to say something.. and so she actually goes to see her...<br /><br />The lonely, traumatized, and both physically and emotionally wounded Deana is more than happy to have someone visit her, but of course, Alicia is anything BUT happy to be seeing her.. Deana attempts to give her a friendly welcome, but of course, Alicia responds with nothing but harsh and hurtful comments and a harsh statement on how she is only here because she is being forced, and has no intention of being friendly with her at all. But sooner or later, that intention will change... (and that\\'s all I\\'ll say :) This is truly one of the most moving movies ever, as well as one of the most dark and disturbing.. Actually, I think I would tie this with \"American History X\" as equally disturbing and moving at the same time...<br /><br />WARNING: Watch this movie at your own risk!! It contains VERY graphic scenes and images! EXCELLENT and criminally under-appreciated movie! I feel so ashamed that I\\'m pretty much the only one that knows about it!', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cf5dac-c429-480a-9545-7d20d1f3bfb9",
   "metadata": {},
   "source": [
    "## Listing 11.3\n",
    "Preprocessing our datasets with a TextVectorization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a904102-ae81-4b24-bd9a-edca24f87851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization( max_tokens=20000, output_mode=\"multi_hot\")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_1gram_train_ds = train_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map( lambda x, y: (text_vectorization(x), y),num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4012ddc-88f4-4dc2-b983-9aa11f6e1a7b",
   "metadata": {},
   "source": [
    "## Listing 11.4\n",
    "Inspecting the output of our binary unigram dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cce6611-6109-4549-813c-2b313be7818d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dbe81e-093b-416a-9518-fdadab483eaf",
   "metadata": {},
   "source": [
    "## Listing 11.5\n",
    "Our model-building utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "712388fd-69bb-46f1-bc5a-e9e1e4fcc960",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe89da-7b46-4357-b918-251f56f4a5b7",
   "metadata": {},
   "source": [
    "## Listing 11.6\n",
    "Training and testing the binary unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "935fbf5e-edf7-49a9-a0d5-d8170fb580b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b102d314-fb12-4597-b908-90c8a5e345b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  1/625 [..............................] - ETA: 15:37 - loss: 0.6977 - accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 21:55:29.153124: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f10388fda30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-14 21:55:29.153164: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2060, Compute Capability 7.5\n",
      "2024-01-14 21:55:29.158210: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-14 21:55:29.170753: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1705287329.207333   29883 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 4s 4ms/step - loss: 0.3948 - accuracy: 0.8353 - val_loss: 0.2835 - val_accuracy: 0.8862\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2672 - accuracy: 0.9011 - val_loss: 0.2858 - val_accuracy: 0.8916\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2331 - accuracy: 0.9177 - val_loss: 0.2989 - val_accuracy: 0.8924\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2189 - accuracy: 0.9222 - val_loss: 0.3200 - val_accuracy: 0.8910\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2130 - accuracy: 0.9316 - val_loss: 0.3344 - val_accuracy: 0.8914\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2018 - accuracy: 0.9362 - val_loss: 0.3461 - val_accuracy: 0.8928\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2012 - accuracy: 0.9342 - val_loss: 0.3563 - val_accuracy: 0.8904\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2067 - accuracy: 0.9370 - val_loss: 0.3686 - val_accuracy: 0.8910\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1973 - accuracy: 0.9384 - val_loss: 0.3754 - val_accuracy: 0.8910\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 6ms/step - loss: 0.1975 - accuracy: 0.9390 - val_loss: 0.3872 - val_accuracy: 0.8912\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2957 - accuracy: 0.8813\n",
      "Test acc: 0.881\n"
     ]
    }
   ],
   "source": [
    "\n",
    "callbacks = [ keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only=True) ]\n",
    "model.fit(binary_1gram_train_ds.cache(), validation_data=binary_1gram_val_ds.cache(), epochs=10, callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e506ced-ae52-45e5-9ccd-28c48e40ae24",
   "metadata": {},
   "source": [
    "## Listing 11.7\n",
    "Configuring the TextVectorization layer to return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99d63f61-0cc2-44ba-91d5-f3fa81a8d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization( ngrams=2,max_tokens=20000,output_mode=\"multi_hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e556974-5c38-4ad8-b676-02fbf6e5c459",
   "metadata": {},
   "source": [
    "## Listing 11.8\n",
    "Training and testing the binary bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb045420-ca4f-4aa0-99dc-ea491a80dfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3674 - accuracy: 0.8468 - val_loss: 0.2765 - val_accuracy: 0.8908\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2332 - accuracy: 0.9189 - val_loss: 0.2768 - val_accuracy: 0.8948\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1955 - accuracy: 0.9360 - val_loss: 0.2905 - val_accuracy: 0.9004\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1767 - accuracy: 0.9444 - val_loss: 0.3083 - val_accuracy: 0.9030\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1693 - accuracy: 0.9493 - val_loss: 0.3236 - val_accuracy: 0.9012\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1567 - accuracy: 0.9538 - val_loss: 0.3504 - val_accuracy: 0.8976\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1642 - accuracy: 0.9542 - val_loss: 0.3538 - val_accuracy: 0.9020\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1581 - accuracy: 0.9574 - val_loss: 0.3614 - val_accuracy: 0.8968\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1549 - accuracy: 0.9585 - val_loss: 0.3727 - val_accuracy: 0.8994\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1519 - accuracy: 0.9597 - val_loss: 0.3866 - val_accuracy: 0.8990\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2793 - accuracy: 0.8910\n",
      "Test acc: 0.891\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map( lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",save_best_only=True)]\n",
    "\n",
    "model.fit(binary_2gram_train_ds.cache(), validation_data=binary_2gram_val_ds.cache(), epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7209abf6-153a-4cd4-8fe0-10db9aff6a2c",
   "metadata": {},
   "source": [
    "## Listing 11.9\n",
    "Configuring the TextVectorization layer to return token counts\n",
    "\n",
    "If you’re doing text classification, knowing how many times a word occurs in a sample\n",
    "is critical: any sufficiently long movie review may contain the word “terrible” regard-\n",
    "less of sentiment, but a review that contains many instances of the word “terrible” is\n",
    "likely a negative one.\n",
    "\n",
    "Here’s how you’d count bigram occurrences with the TextVectorization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d86141ae-f08b-4390-a5b8-6c3d538d079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization( ngrams=2, max_tokens=20000, output_mode=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2de95-b51b-4b5b-81e2-ea1012b171e6",
   "metadata": {},
   "source": [
    "## Listing 11.10 \n",
    "Configuring TextVectorization to return TF-IDF-weighted outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "303903f2-4514-4ca4-90d9-42706632a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(ngrams=2, max_tokens=20000, output_mode=\"tf_idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218bef0a-6023-4cd5-b1a3-953480e10e18",
   "metadata": {},
   "source": [
    "## Listing 11.11 \n",
    "Training and testing the TF-IDF bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d36501d1-c299-43ab-9944-d18ee28284d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.4637 - accuracy: 0.7952 - val_loss: 0.2919 - val_accuracy: 0.8828\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3071 - accuracy: 0.8766 - val_loss: 0.2912 - val_accuracy: 0.8906\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2797 - accuracy: 0.8881 - val_loss: 0.3013 - val_accuracy: 0.8908\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2544 - accuracy: 0.9006 - val_loss: 0.3172 - val_accuracy: 0.8916\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2369 - accuracy: 0.9062 - val_loss: 0.3340 - val_accuracy: 0.8842\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2293 - accuracy: 0.9114 - val_loss: 0.3513 - val_accuracy: 0.8850\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2147 - accuracy: 0.9141 - val_loss: 0.3735 - val_accuracy: 0.8754\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2085 - accuracy: 0.9201 - val_loss: 0.3661 - val_accuracy: 0.8888\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 6ms/step - loss: 0.1973 - accuracy: 0.9219 - val_loss: 0.4023 - val_accuracy: 0.8848\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1880 - accuracy: 0.9254 - val_loss: 0.3899 - val_accuracy: 0.8820\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2843 - accuracy: 0.8943\n",
      "Test acc: 0.894\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y),num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",save_best_only=True)]\n",
    "model.fit(tfidf_2gram_train_ds.cache(), validation_data=tfidf_2gram_val_ds.cache(), epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb5066a-533d-431d-b24d-3aca4361a22a",
   "metadata": {},
   "source": [
    "## Listing 11.12 \n",
    "Preparing integer sequence datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ad80d17-a24d-45eb-bdfd-4380d91ffcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization( max_tokens=max_tokens, output_mode=\"int\", output_sequence_length=max_length)\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a04cee-0005-496a-9c60-bd534888d200",
   "metadata": {},
   "source": [
    "## Listing 11.13 \n",
    "A sequence model built on one-hot encoded vector sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a550ec6-738d-4a54-9753-ddc974257b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 64)                5128448   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5128513 (19.56 MB)\n",
      "Trainable params: 5128513 (19.56 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd51e80-3014-480b-b389-19674aadc18b",
   "metadata": {},
   "source": [
    "## Listing 11.14 \n",
    "## Was not able to execute this listing\n",
    "## Listings 11.21 - 11.24 Remains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc5c6353-38fd-4d28-a007-b3d3638d7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks = [ keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\", save_best_only=True) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da28a0b0-0953-4a36-8558-9b966ba90dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4768d8c-f61f-4bf2-ba39-e150905fe64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
    "#print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
