{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22bfcc7e",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/erinijapranckeviciene/MF54609_18981_1_20241/blob/main/FC_book_Ch8_Dogs_vs_Cats_CNN_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fefc06-1e66-4e2e-b288-7d0c9a4689c5",
   "metadata": {
    "id": "d0fefc06-1e66-4e2e-b288-7d0c9a4689c5"
   },
   "source": [
    "# Dogs vs Cats classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dVCl8VNaazMF",
   "metadata": {
    "id": "dVCl8VNaazMF"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruGrTcJ_qYRd",
   "metadata": {
    "id": "ruGrTcJ_qYRd"
   },
   "source": [
    "### In the github is a reduced size data set because it takes very long time to train in the colaboratory.\n",
    "\n",
    "Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y3OW_kPOjiVQ",
   "metadata": {
    "id": "y3OW_kPOjiVQ"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/erinijapranckeviciene/MF54609_18981_1_20241/raw/refs/heads/main/datasets/cats_vs_dogs_very_small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LLBJ3RWlkXrp",
   "metadata": {
    "id": "LLBJ3RWlkXrp"
   },
   "outputs": [],
   "source": [
    "#!pwd\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mo5nghTdqoXy",
   "metadata": {
    "id": "mo5nghTdqoXy"
   },
   "source": [
    "### Unzip tha dataset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GcIM80Dek_PP",
   "metadata": {
    "id": "GcIM80Dek_PP"
   },
   "outputs": [],
   "source": [
    "!unzip cats_vs_dogs_very_small.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798f46d-8e6b-4e3c-837c-9163b05a13ec",
   "metadata": {
    "id": "c798f46d-8e6b-4e3c-837c-9163b05a13ec"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf4f0f-3132-4a81-a545-5205ba19fe3c",
   "metadata": {
    "id": "f9bf4f0f-3132-4a81-a545-5205ba19fe3c"
   },
   "source": [
    "### Cifar10 dataset is in tensorflow if needed;\n",
    "\n",
    "After working with the code for classification Dogs vs Cats let's try to adapt ir to other datasets.\n",
    "In tensorflow datasets https://www.tensorflow.org/datasets/catalog/overview, see the docs how to retrieve it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b4dd7-55b8-4885-82ef-89028e54c88f",
   "metadata": {
    "id": "546b4dd7-55b8-4885-82ef-89028e54c88f"
   },
   "outputs": [],
   "source": [
    "#### Made as markdown not to run\n",
    "import os, shutil, pathlib\n",
    "\n",
    "original_dir = pathlib.Path(\"kaggle/train\")\n",
    "new_base_dir = pathlib.Path(\"cats_vs_dogs_very_small\")\n",
    "\n",
    "\"\"\"\n",
    "This is to prepare data from images from unpacked *zip\n",
    "Uncomment if running for the first time\n",
    "def make_subset(subset_name, start_index, end_index):\n",
    "    for category in (\"cat\", \"dog\"):\n",
    "        dir = new_base_dir / subset_name / category\n",
    "        os.makedirs(dir)\n",
    "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
    "        for fname in fnames:\n",
    "            shutil.copyfile(src=original_dir / fname, dst=dir / fname)\n",
    "\n",
    "make_subset(\"train\", start_index=0, end_index=1000)\n",
    "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
    "make_subset(\"test\", start_index=1500, end_index=2500)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd7570-a599-496e-baeb-1dbd4e59e25c",
   "metadata": {
    "id": "56fd7570-a599-496e-baeb-1dbd4e59e25c"
   },
   "source": [
    "## Listing 8.7\n",
    "Instantiating a small convnet for dogs vs. cats classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3c2ab1-22ad-484b-8149-9f7ef26c7a98",
   "metadata": {
    "id": "af3c2ab1-22ad-484b-8149-9f7ef26c7a98"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae9d31-bbbe-4ae1-9cac-e8a04a566f87",
   "metadata": {
    "id": "b2ae9d31-bbbe-4ae1-9cac-e8a04a566f87"
   },
   "source": [
    "## Listing 8.8\n",
    "Configuring the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580d1db-540b-4e63-8d25-6e4a2796dc8e",
   "metadata": {
    "id": "b580d1db-540b-4e63-8d25-6e4a2796dc8e"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "optimizer=\"rmsprop\",\n",
    "metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb22e44-e541-4dab-8cbf-882c19c37349",
   "metadata": {
    "id": "aeb22e44-e541-4dab-8cbf-882c19c37349"
   },
   "source": [
    "## Listing 8.9\n",
    "Using image_dataset_from_directory to read images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d7505-7876-4fdc-a30b-c8635360011b",
   "metadata": {
    "id": "997d7505-7876-4fdc-a30b-c8635360011b"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "train_dataset = image_dataset_from_directory( new_base_dir / \"train\", image_size=(180, 180), batch_size=20)\n",
    "validation_dataset = image_dataset_from_directory(new_base_dir / \"validation\", image_size=(180, 180), batch_size=20)\n",
    "test_dataset = image_dataset_from_directory(new_base_dir / \"test\",image_size=(180, 180), batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5636e2-d9c7-42c1-9ac3-adee1ec7f7f1",
   "metadata": {
    "id": "0b5636e2-d9c7-42c1-9ac3-adee1ec7f7f1"
   },
   "source": [
    "## Listing 8.10\n",
    "Displaying the shapes of the data and labels yielded by the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59f2b6-7f6d-4b00-bc65-4cec11eb4366",
   "metadata": {
    "id": "fe59f2b6-7f6d-4b00-bc65-4cec11eb4366"
   },
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_dataset:\n",
    "    print(\"data batch shape:\", data_batch.shape)\n",
    "    print(\"labels batch shape:\", labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942407c9-272a-4d94-861f-4c7957a85559",
   "metadata": {
    "id": "942407c9-272a-4d94-861f-4c7957a85559"
   },
   "source": [
    "## Listing 8.11\n",
    "Fitting the model using a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad8d93-ea0a-42e9-b01e-12f1aa133552",
   "metadata": {
    "id": "1bad8d93-ea0a-42e9-b01e-12f1aa133552"
   },
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint( filepath=\"convnet_from_scratch.keras\",\n",
    "                                             save_best_only=True, monitor=\"val_loss\") ]\n",
    "\n",
    "# save_best_only in ModelCheckpoint saves the last best in the filepath\n",
    "# The EarlyStopping callback stops the training early, while ModelCheckpoint continues\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "\n",
    "## Takes very long time to train, reduce to three epochs\n",
    "history = model.fit( train_dataset, epochs=15, validation_data=validation_dataset, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c6cdab-f380-4d03-868c-51b571845d62",
   "metadata": {
    "id": "14c6cdab-f380-4d03-868c-51b571845d62"
   },
   "source": [
    "## Listing 8.12\n",
    "Displaying curves of loss and accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684277f-641b-4829-9ffe-5de09cdec014",
   "metadata": {
    "id": "6684277f-641b-4829-9ffe-5de09cdec014"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401f5368-0907-4d18-ae3c-b990cbd5bc92",
   "metadata": {
    "id": "401f5368-0907-4d18-ae3c-b990cbd5bc92"
   },
   "source": [
    "## Listing 8.13\n",
    "Evaluating the model on the test set . The checkpoints saved the best before overfitting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80101486-e589-4c2f-bd4e-42409de53a6e",
   "metadata": {
    "id": "80101486-e589-4c2f-bd4e-42409de53a6e"
   },
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f85dd6-8ffb-4c28-9334-57dd12c66e41",
   "metadata": {
    "id": "c4f85dd6-8ffb-4c28-9334-57dd12c66e41"
   },
   "source": [
    "## Listing 8.14\n",
    "Define a data augmentation stage to add to an image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c355610a-e286-43af-8888-28ac54076a09",
   "metadata": {
    "id": "c355610a-e286-43af-8888-28ac54076a09"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([ layers.RandomFlip(\"horizontal\"),\n",
    "                                      layers.RandomRotation(0.1),\n",
    "                                      layers.RandomZoom(0.2),])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad9f07-9cbb-4c3b-acd0-25296e8191f4",
   "metadata": {
    "id": "25ad9f07-9cbb-4c3b-acd0-25296e8191f4"
   },
   "source": [
    "## Listing 8.15\n",
    "Displaying some randomly augmented training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f4906-4df5-45f2-8352-66e84152a75b",
   "metadata": {
    "id": "db5f4906-4df5-45f2-8352-66e84152a75b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_dataset.take(2):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax=plt.subplot(3,3,i+1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9feb4-230b-4a91-b34b-41c7f7b20107",
   "metadata": {
    "id": "3ee9feb4-230b-4a91-b34b-41c7f7b20107"
   },
   "source": [
    "## Listing 8.16\n",
    "Defining a new convnet that includes image augmentation and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7439a3-30be-43dc-a086-7f2db4f3eeec",
   "metadata": {
    "id": "ba7439a3-30be-43dc-a086-7f2db4f3eeec"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "optimizer=\"rmsprop\",\n",
    "metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec6b6d-7278-4054-9be0-4be25796005d",
   "metadata": {
    "id": "53ec6b6d-7278-4054-9be0-4be25796005d"
   },
   "source": [
    "## Listing 8.17\n",
    "Training the regularized convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0c244-595b-416d-9cc5-3ce5f650d517",
   "metadata": {
    "id": "79f0c244-595b-416d-9cc5-3ce5f650d517"
   },
   "outputs": [],
   "source": [
    "callbacks = [ keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\") ]\n",
    "\n",
    "history = model.fit( train_dataset,\n",
    "                    epochs=50,\n",
    "                    validation_data=validation_dataset,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75997b-d1d3-49ca-aaec-900faf5d8270",
   "metadata": {
    "id": "cb75997b-d1d3-49ca-aaec-900faf5d8270"
   },
   "source": [
    "## Visualize again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368da699-071f-45e7-bb0e-ba1862315e90",
   "metadata": {
    "id": "368da699-071f-45e7-bb0e-ba1862315e90"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf1bf3-7bc9-4457-b342-bdd3a3e289f9",
   "metadata": {
    "id": "5abf1bf3-7bc9-4457-b342-bdd3a3e289f9"
   },
   "source": [
    "## Listing 8.18\n",
    "Evaluating the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f40e6-0196-4806-b8b6-6a68f5165c0e",
   "metadata": {
    "id": "df4f40e6-0196-4806-b8b6-6a68f5165c0e"
   },
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model( \"convnet_from_scratch_with_augmentation.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65254892-68c5-4299-a269-55bff72915a9",
   "metadata": {
    "id": "65254892-68c5-4299-a269-55bff72915a9"
   },
   "source": [
    "# Using pre-trained models FC book 8.3\n",
    "### You can practice this either individually or through a discussion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f142e35-44e5-4c6c-b1cc-449194d80e4d",
   "metadata": {
    "id": "3f142e35-44e5-4c6c-b1cc-449194d80e4d"
   },
   "source": [
    "## 9.3.5 Putting it together: A mini Xception-like model\n",
    "This is a resemblance to the Inception model in Juozapas presentation\n",
    "Using the same Cats_vs_Dogs dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fdd62b-fe89-4b85-b98b-b9926ca63020",
   "metadata": {
    "id": "a1fdd62b-fe89-4b85-b98b-b9926ca63020"
   },
   "source": [
    "As a reminder, here are the convnet architecture principles you’ve learned so far:\n",
    "\n",
    "    * Your model should be organized into repeated blocks of layers, usually made of multiple convolution layers and a max pooling layer.\n",
    "    * The number of filters in your layers should increase as the size of the spatial feature maps decreases.\n",
    "    * Deep and narrow is better than broad and shallow.\n",
    "    * Introducing residual connections around blocks of layers helps you train deeper networks.\n",
    "    * It can be beneficial to introduce batch normalization layers after your convolution layers.\n",
    "    * It can be beneficial to replace Conv2D layers with SeparableConv2D layers, which are more parameter-efficient.\n",
    "\n",
    "This is implemented in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd37809d-7439-4e87-ab95-178bfcf39e69",
   "metadata": {
    "id": "cd37809d-7439-4e87-ab95-178bfcf39e69"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "\n",
    "# data augmentation is from previous example\n",
    "data_augmentation = keras.Sequential([ layers.RandomFlip(\"horizontal\"),\n",
    "                                      layers.RandomRotation(0.1),\n",
    "                                      layers.RandomZoom(0.2),])\n",
    "\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32, 64, 128, 256, 512]:\n",
    "    residual = x\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "    residual = layers.Conv2D(\n",
    "    size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
    "# The Dropout layer randomly sets input units to 0 with a frequency of rate\n",
    "# at each step during training time,which helps prevent overfitting.\n",
    "# Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
    "\n",
    "# To prune the model is a different approach\n",
    "# https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae789b0-533c-43f6-932a-04088f6a0e95",
   "metadata": {
    "id": "aae789b0-533c-43f6-932a-04088f6a0e95"
   },
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58fb6f-fc0d-469b-942f-78534e4b2154",
   "metadata": {
    "id": "ff58fb6f-fc0d-469b-942f-78534e4b2154"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "optimizer=\"rmsprop\",\n",
    "metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ad91e-b3d4-4527-a1be-4644149a6cfa",
   "metadata": {
    "id": "346ad91e-b3d4-4527-a1be-4644149a6cfa"
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96130b9a-63f1-41d8-b4ea-c42926586ff0",
   "metadata": {
    "id": "96130b9a-63f1-41d8-b4ea-c42926586ff0"
   },
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.ModelCheckpoint( filepath=\"xception-like.keras\",\n",
    "                                             save_best_only=True, monitor=\"val_loss\") ]\n",
    "\n",
    "# save_best_only in ModelCheckpoint saves the last best in the filepath\n",
    "# The EarlyStopping callback stops the training early, while ModelCheckpoint continues\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "\n",
    "history = model.fit( train_dataset, epochs=30, validation_data=validation_dataset, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92990a7-9f85-40ad-8b94-9312a66b7023",
   "metadata": {
    "id": "e92990a7-9f85-40ad-8b94-9312a66b7023"
   },
   "source": [
    "#### Visualize again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9798012-3320-458e-a01c-739cdf152e0e",
   "metadata": {
    "id": "b9798012-3320-458e-a01c-739cdf152e0e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024211c-b35e-4ec6-a82f-538e08b66226",
   "metadata": {
    "id": "f024211c-b35e-4ec6-a82f-538e08b66226"
   },
   "source": [
    "#### Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a26795-b6c9-41d9-b727-dd40f0840e25",
   "metadata": {
    "id": "b4a26795-b6c9-41d9-b727-dd40f0840e25"
   },
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model( \"xception-like.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf2be2-bf34-42db-a5df-57324acc0f5e",
   "metadata": {
    "id": "99cf2be2-bf34-42db-a5df-57324acc0f5e"
   },
   "source": [
    "## Continuing with 9.4 Interpreting what Covnets learn\n",
    "\n",
    "This is very interesting.\n",
    "\n",
    "#### 1. Visualizing intermediate convnet outputs (intermediate activations)\n",
    "Useful for understanding how successive convnet layers transform their input, and for getting a\n",
    "first idea of the meaning of individual convnet filters\n",
    "\n",
    "#### 2. Visualizing convnet filters\n",
    "Useful for understanding precisely what visual pattern\n",
    "or concept each filter in a convnet is receptive to\n",
    "\n",
    "#### 3. Visualizing heatmaps of class activation in an image\n",
    "Useful for understanding which parts of an image were identified as belonging to a given class, thus allowing you to localize objects in images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d479a-9f34-4ea9-beb1-6d4a0738f1e1",
   "metadata": {
    "id": "227d479a-9f34-4ea9-beb1-6d4a0738f1e1"
   },
   "source": [
    "## 9.4.1 Visualizing intermediate activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17873322-3604-4cb4-95e5-0dc2f97c9699",
   "metadata": {
    "id": "17873322-3604-4cb4-95e5-0dc2f97c9699"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"convnet_from_scratch_with_augmentation.keras\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05992853-bf1b-4532-96b0-f93baaaf0284",
   "metadata": {
    "id": "05992853-bf1b-4532-96b0-f93baaaf0284"
   },
   "source": [
    "## Listing 9.6\n",
    "Preprocessing a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3971c7-87e3-4ab3-87b5-af7027f5aa87",
   "metadata": {
    "id": "6b3971c7-87e3-4ab3-87b5-af7027f5aa87"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Test image\n",
    "img_path = keras.utils.get_file( fname=\"cat.jpg\", origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\")\n",
    "\n",
    "def get_img_array(img_path, target_size):\n",
    "    img = keras.utils.load_img( img_path, target_size=target_size)\n",
    "    array = keras.utils.img_to_array(img)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "img_tensor = get_img_array(img_path, target_size=(180, 180))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467cf36-ab0b-445d-b3cf-c31620dd835c",
   "metadata": {
    "id": "d467cf36-ab0b-445d-b3cf-c31620dd835c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img_tensor[0].astype(\"uint8\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc298f53-1524-49e1-86cb-51a01f0e0289",
   "metadata": {
    "id": "bc298f53-1524-49e1-86cb-51a01f0e0289"
   },
   "source": [
    "## Listing 9.8\n",
    "Instantiating a model that returns layer activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a82b9c-7c18-4f93-acd3-56cc6f8f7883",
   "metadata": {
    "id": "55a82b9c-7c18-4f93-acd3-56cc6f8f7883"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "layer_outputs = []\n",
    "layer_names = []\n",
    "\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):\n",
    "        layer_outputs.append(layer.output)\n",
    "        layer_names.append(layer.name)\n",
    "\n",
    "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d37747-2977-44b4-bb41-a62731f400b6",
   "metadata": {
    "id": "d2d37747-2977-44b4-bb41-a62731f400b6"
   },
   "source": [
    "## Listing 9.9\n",
    "Using the model to compute layer activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb0055f-a7f6-443b-8715-3d7b13c24109",
   "metadata": {
    "id": "8eb0055f-a7f6-443b-8715-3d7b13c24109"
   },
   "outputs": [],
   "source": [
    "activations = activation_model.predict(img_tensor)\n",
    "\n",
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5efc9-7b49-4041-bc5f-6d6f108c5155",
   "metadata": {
    "id": "e8e5efc9-7b49-4041-bc5f-6d6f108c5155"
   },
   "source": [
    "## Listing 9.10\n",
    "Visualizing the fifth channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b57f85-8f46-4406-b3d3-6fbae73f0ad0",
   "metadata": {
    "id": "85b57f85-8f46-4406-b3d3-6fbae73f0ad0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(first_layer_activation[0, :, :, 5], cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2c22b9-b082-447d-a1ea-58d2bdd69e5b",
   "metadata": {
    "id": "5f2c22b9-b082-447d-a1ea-58d2bdd69e5b"
   },
   "source": [
    "## Listing 9.11\n",
    "Visualizing every channel in every intermediate activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f41e9-665e-4275-99f5-7dcbb2b6ef78",
   "metadata": {
    "id": "f33f41e9-665e-4275-99f5-7dcbb2b6ef78"
   },
   "outputs": [],
   "source": [
    "images_per_row = 16\n",
    "\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    n_features = layer_activation.shape[-1]\n",
    "    size = layer_activation.shape[1]\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros(((size + 1) * n_cols - 1, images_per_row * (size + 1) - 1))\n",
    "\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_index = col * images_per_row + row\n",
    "            channel_image = layer_activation[0, :, :, channel_index].copy()\n",
    "            if channel_image.sum() != 0:\n",
    "                channel_image -= channel_image.mean()\n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n",
    "            display_grid[ col * (size + 1): (col + 1) * size + col, row * (size + 1) : (row + 1) * size + row] = channel_image\n",
    "\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6f288-2a2c-43d3-ae5b-94f4636e2805",
   "metadata": {
    "id": "dea6f288-2a2c-43d3-ae5b-94f4636e2805"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d02da8c-017f-43ce-a198-ecf551dfdbb4",
   "metadata": {
    "id": "5d02da8c-017f-43ce-a198-ecf551dfdbb4"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"xception-like.keras\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b6600-dbd5-48f6-b09e-8ef8330af70c",
   "metadata": {
    "id": "872b6600-dbd5-48f6-b09e-8ef8330af70c"
   },
   "source": [
    "## 9.4.2 Visualizing convnet filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22cb4aa-7fa4-4504-bc3c-ddb024fa4bf1",
   "metadata": {
    "id": "e22cb4aa-7fa4-4504-bc3c-ddb024fa4bf1"
   },
   "source": [
    "## Listing 9.12\n",
    "Instantiating the Xception convolutional base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0255c4f2-5e83-475d-a9c5-7fd31349dcf7",
   "metadata": {
    "id": "0255c4f2-5e83-475d-a9c5-7fd31349dcf7"
   },
   "outputs": [],
   "source": [
    "model = keras.applications.xception.Xception( weights=\"imagenet\", include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8239f4-8e8c-4f9d-882a-20b47c090a89",
   "metadata": {
    "id": "fa8239f4-8e8c-4f9d-882a-20b47c090a89"
   },
   "source": [
    "## Listing 9.13\n",
    "Printing the names of all convolutional layers in Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d161b26-dbaa-486b-96ad-f72f6a525a06",
   "metadata": {
    "id": "4d161b26-dbaa-486b-96ad-f72f6a525a06"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):\n",
    "        print(layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a633acb-1bac-445f-bcef-94c4491bf156",
   "metadata": {
    "id": "2a633acb-1bac-445f-bcef-94c4491bf156"
   },
   "source": [
    "## Listing 9.14\n",
    "Creating a feature extractor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78b91e-36ce-4109-8c23-1878799fed43",
   "metadata": {
    "id": "2f78b91e-36ce-4109-8c23-1878799fed43"
   },
   "outputs": [],
   "source": [
    "layer_name = \"block3_sepconv1\"\n",
    "layer = model.get_layer(name=layer_name)\n",
    "feature_extractor = keras.Model(inputs=model.input, outputs=layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29237b9e-baef-4524-af08-5ab152277aba",
   "metadata": {
    "id": "29237b9e-baef-4524-af08-5ab152277aba"
   },
   "source": [
    "## Listing 9.15\n",
    "Using the feature extractor\n",
    "\n",
    "To use this model, simply call it on some input data (note that Xception requires\n",
    "inputs to be preprocessed via the keras.applications.xception.preprocess_input\n",
    "function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15838a-c527-4f43-9dc5-2ffc5a552468",
   "metadata": {
    "id": "8a15838a-c527-4f43-9dc5-2ffc5a552468"
   },
   "outputs": [],
   "source": [
    "activation = feature_extractor( keras.applications.xception.preprocess_input(img_tensor) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871171b-7b47-4190-9952-8e11ba37440c",
   "metadata": {
    "id": "0871171b-7b47-4190-9952-8e11ba37440c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def compute_loss(image, filter_index):\n",
    "    activation = feature_extractor(image)\n",
    "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
    "    return tf.reduce_mean(filter_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db04cc2-8da7-4d35-a0b3-29dbfbe0f634",
   "metadata": {
    "id": "1db04cc2-8da7-4d35-a0b3-29dbfbe0f634"
   },
   "source": [
    "## Listing 9.16\n",
    "Loss maximization via stochastic gradient ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64037bf-edd7-4721-a542-eb74328b0c8c",
   "metadata": {
    "id": "c64037bf-edd7-4721-a542-eb74328b0c8c"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "\n",
    "def gradient_ascent_step(image, filter_index, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        loss = compute_loss(image, filter_index)\n",
    "    grads = tape.gradient(loss, image)\n",
    "    grads = tf.math.l2_normalize(grads)\n",
    "    image += learning_rate * grads\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cdf0ff-a79d-4163-8dbc-37310f7ccd04",
   "metadata": {
    "id": "61cdf0ff-a79d-4163-8dbc-37310f7ccd04"
   },
   "source": [
    "## Listing 9.17\n",
    "Function to generate filter visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68a846-7932-41fb-b090-a1c9933b0910",
   "metadata": {
    "id": "6c68a846-7932-41fb-b090-a1c9933b0910"
   },
   "outputs": [],
   "source": [
    "img_width = 200\n",
    "img_height = 200\n",
    "\n",
    "def generate_filter_pattern(filter_index):\n",
    "    iterations = 30\n",
    "    learning_rate = 10.\n",
    "    image = tf.random.uniform( minval=0.4, maxval=0.6, shape=(1, img_width, img_height, 3))\n",
    "    for i in range(iterations):\n",
    "        image = gradient_ascent_step(image, filter_index, learning_rate)\n",
    "    return image[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1629abc-7294-4c67-8324-bd0b64daf37c",
   "metadata": {
    "id": "e1629abc-7294-4c67-8324-bd0b64daf37c"
   },
   "source": [
    "## Listing 9.18\n",
    "Utility function to convert a tensor into a valid image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df35e6d1-a8c4-4e93-a6c4-e665b7087f9a",
   "metadata": {
    "id": "df35e6d1-a8c4-4e93-a6c4-e665b7087f9a"
   },
   "outputs": [],
   "source": [
    "def deprocess_image(image):\n",
    "    image -= image.mean()\n",
    "    image /= image.std()\n",
    "    image *= 64\n",
    "    image += 128\n",
    "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
    "    image = image[25:-25, 25:-25, :]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3ebdd1-bdaf-4151-8064-bd796b085f85",
   "metadata": {
    "id": "9e3ebdd1-bdaf-4151-8064-bd796b085f85"
   },
   "outputs": [],
   "source": [
    "plt.axis(\"off\")\n",
    "plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271dccd-3ba5-4c4b-b0c2-3709a3f2fbaf",
   "metadata": {
    "id": "3271dccd-3ba5-4c4b-b0c2-3709a3f2fbaf"
   },
   "source": [
    "## Listing 9.19\n",
    "Generating a grid of all filter response patterns in a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce1888c-ae0a-4d0b-887d-a73f42c40023",
   "metadata": {
    "id": "2ce1888c-ae0a-4d0b-887d-a73f42c40023"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "all_images = []\n",
    "for filter_index in range(64):\n",
    "    print(f\"Processing filter {filter_index}\")\n",
    "    image = deprocess_image(generate_filter_pattern(filter_index) )\n",
    "    all_images.append(image)\n",
    "\n",
    "margin = 5\n",
    "n = 8\n",
    "cropped_width = img_width - 25 * 2\n",
    "cropped_height = img_height - 25 * 2\n",
    "width = n * cropped_width + (n - 1) * margin\n",
    "height = n * cropped_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        image = all_images[i * n + j]\n",
    "        row_start = (cropped_width + margin) * i\n",
    "        row_end = (cropped_width + margin) * i + cropped_width\n",
    "        column_start = (cropped_height + margin) * j\n",
    "        column_end = (cropped_height + margin) * j + cropped_height\n",
    "        stitched_filters[ row_start: row_end,column_start: column_end, :] = image\n",
    "\n",
    "keras.utils.save_img(f\"filters_for_layer_{layer_name}.png\", stitched_filters)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2634f48c-3cab-409f-b289-dbf236e1e10b",
   "metadata": {
    "id": "2634f48c-3cab-409f-b289-dbf236e1e10b"
   },
   "source": [
    "## 9.4.3\n",
    "Visualizing heatmaps of class activation\n",
    "\n",
    "Complete if you have time."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
