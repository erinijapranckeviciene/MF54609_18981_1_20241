{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNyQJS160MVjutSy/NeJ4u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erinijapranckeviciene/MF54609_18981_1_20241/blob/main/FC_Chapter11_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnMIiIp0_jyI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare IMDB data as before"
      ],
      "metadata": {
        "id": "9DgKEIRI2sh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/erinijapranckeviciene/MF54609_18981_1_20241/raw/refs/heads/main/datasets/RNN/aclImdb.zip\n",
        "!unzip -qq aclImdb.zip"
      ],
      "metadata": {
        "id": "IPiaF2K92y-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "\n",
        "batch_size = 32\n",
        "train_ds = keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size )\n",
        "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
        "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
      ],
      "metadata": {
        "id": "1_yx_zFA20QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### text only ds"
      ],
      "metadata": {
        "id": "CpCkp1Qd3RLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function returns only data part without target\n",
        "# to create a new dataset that will be used to create dictionary\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "for inputs in text_only_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    break"
      ],
      "metadata": {
        "id": "qRsqcxUY3QHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare int representation dataset"
      ],
      "metadata": {
        "id": "ymdvZyY34heG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# max_length is a length of the vector that encodes text\n",
        "# 250 size gave ~0.85 on test data\n",
        "# 600 with 4 LSTM units does not train at all\n",
        "# the reviews are about 300 words\n",
        "max_length = 300\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization( max_tokens=max_tokens, output_mode=\"int\", output_sequence_length=max_length)\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "H5sYnQr54mar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify"
      ],
      "metadata": {
        "id": "UkzkMk4Q4yxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in int_train_ds:\n",
        "    print(inputs.shape)\n",
        "    print(inputs[0])\n",
        "    print(targets[0])\n",
        "    # With this test_input variable verify tf.one_hot() transformation\n",
        "    test_input=inputs[0]\n",
        "    break"
      ],
      "metadata": {
        "id": "grzJkbMa40o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers  F.Chollet Chapter 11\n",
        "\n",
        "#### 11.4 The Transformer architecture\n",
        "\n",
        "...\"Transformers were introduced in the seminal paper “Attention is all you need” by Vaswani et al. The gist of the paper is right there in the title: as it turned out, a simple mechanism called “neural attention” could be used to build powerful sequence models that didn’t feature any recurrent layers or convolution layers.\"..."
      ],
      "metadata": {
        "id": "HcBhro6t_o2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.21 Transformer encoder implemented as a subclassed Layer"
      ],
      "metadata": {
        "id": "R2AKHrqExCpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    if mask is not None:\n",
        "        mask=mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(\n",
        "        inputs, inputs, attention_mask=mask\n",
        "    )\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim,\n",
        "    })\n",
        "    return config\n",
        ""
      ],
      "metadata": {
        "id": "b4Wabfh4_6KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.22 Using the Transformer encoder for text classification"
      ],
      "metadata": {
        "id": "YmTqWI9V5REL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "oPRMSEZhxERm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.23 Training and evaluating the Transformer encoder based model"
      ],
      "metadata": {
        "id": "PTGLg1XB5w7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [ keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\", save_best_only=True) ]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"transformer_encoder.keras\", custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n"
      ],
      "metadata": {
        "id": "CZeHNA4t5y7M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}