{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPfNTLRp17B7VIQvIXJd9kJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erinijapranckeviciene/MF54609_18981_1_20241/blob/main/FC_Chapter11_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnMIiIp0_jyI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BTW, Recalling the effect of validation error being less than training\n",
        "\n",
        "https://pyimagesearch.com/2019/10/14/why-is-my-validation-loss-lower-than-my-training-loss/#:~:text=Regularization%20methods%20often%20sacrifice%20training,lower%20than%20your%20training%20loss."
      ],
      "metadata": {
        "id": "dYcA0HOsaYPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare IMDB data as before"
      ],
      "metadata": {
        "id": "9DgKEIRI2sh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/erinijapranckeviciene/MF54609_18981_1_20241/raw/refs/heads/main/datasets/RNN/aclImdb.zip\n",
        "!unzip -qq aclImdb.zip"
      ],
      "metadata": {
        "id": "IPiaF2K92y-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "\n",
        "batch_size = 32\n",
        "train_ds = keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size )\n",
        "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
        "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
      ],
      "metadata": {
        "id": "1_yx_zFA20QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### text only ds"
      ],
      "metadata": {
        "id": "CpCkp1Qd3RLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function returns only data part without target\n",
        "# to create a new dataset that will be used to create dictionary\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "for inputs in text_only_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    break"
      ],
      "metadata": {
        "id": "qRsqcxUY3QHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare int representation dataset"
      ],
      "metadata": {
        "id": "ymdvZyY34heG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# max_length is a length of the vector that encodes text\n",
        "# 250 size gave ~0.85 on test data\n",
        "# 600 with 4 LSTM units does not train at all\n",
        "# the reviews are about 300 words\n",
        "max_length = 300\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization( max_tokens=max_tokens, output_mode=\"int\", output_sequence_length=max_length)\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "H5sYnQr54mar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify"
      ],
      "metadata": {
        "id": "UkzkMk4Q4yxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in int_train_ds:\n",
        "    print(inputs.shape)\n",
        "    print(inputs[0])\n",
        "    print(targets[0])\n",
        "    # With this test_input variable verify tf.one_hot() transformation\n",
        "    test_input=inputs[0]\n",
        "    break"
      ],
      "metadata": {
        "id": "grzJkbMa40o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers  F.Chollet Chapter 11\n",
        "\n",
        "#### 11.4 The Transformer architecture\n",
        "\n",
        "...\"Transformers were introduced in the seminal paper “Attention is all you need” by Vaswani et al. The gist of the paper is right there in the title: as it turned out, a simple mechanism called “neural attention” could be used to build powerful sequence models that didn’t feature any recurrent layers or convolution layers.\"..."
      ],
      "metadata": {
        "id": "HcBhro6t_o2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.21 Transformer encoder implemented as a subclassed Layer"
      ],
      "metadata": {
        "id": "R2AKHrqExCpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    if mask is not None:\n",
        "        mask=mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(\n",
        "        inputs, inputs, attention_mask=mask\n",
        "    )\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim,\n",
        "    })\n",
        "    return config\n"
      ],
      "metadata": {
        "id": "b4Wabfh4_6KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.22 Using the Transformer encoder for text classification"
      ],
      "metadata": {
        "id": "YmTqWI9V5REL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "oPRMSEZhxERm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.23 Training and evaluating the Transformer encoder based model"
      ],
      "metadata": {
        "id": "PTGLg1XB5w7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [ keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\", save_best_only=True) ]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"transformer_encoder.keras\", custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n"
      ],
      "metadata": {
        "id": "CZeHNA4t5y7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.24 Implementing positional embedding as a subclassed layer"
      ],
      "metadata": {
        "id": "FB1hY2un_Xf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(\n",
        "        input_dim=input_dim, output_dim=output_dim)\n",
        "    self.position_embeddings = layers.Embedding(\n",
        "        input_dim=sequence_length, output_dim=output_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  #def compute_mask(self, inputs, mask=None):\n",
        "  #  return tf.math.not_equal(inputs, 0) #it was a mistake correction below\n",
        "    #return layers.Lambda(lambda x: tf.math.not_equal(x, 0))(inputs)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "      \"output_dim\": self.output_dim,\n",
        "      \"sequence_length\": self.sequence_length,\n",
        "      \"input_dim\": self.input_dim,\n",
        "      })\n",
        "    return config"
      ],
      "metadata": {
        "id": "u-39eAre_ZP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.25 Combining the Transformer encoder with positional embedding"
      ],
      "metadata": {
        "id": "M9n0hMnRAWA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "KHL_xQcrAesW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change the dataset - the sequence length is 600\n"
      ],
      "metadata": {
        "id": "qzlCGDJRVwGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = layers.TextVectorization( max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length)\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "jnkNGDrFV2qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute the model"
      ],
      "metadata": {
        "id": "c0t3uw-TFB89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\", save_best_only=True)]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "\n"
      ],
      "metadata": {
        "id": "UwFoflt2FGGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"full_transformer_encoder.keras\", custom_objects={\"TransformerEncoder\": TransformerEncoder, \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "yv5z_bcBYCD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.5.1 A machine translation example"
      ],
      "metadata": {
        "id": "I9WUd_ATfqjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ],
      "metadata": {
        "id": "AHiECLnDgGh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parse the file\n"
      ],
      "metadata": {
        "id": "GmdtCce5gqhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "\n",
        "for line in lines:\n",
        "  eng, spa = line.split(\"\\t\")\n",
        "  spa = \"[start] \" + spa + \" [end]\"\n",
        "  text_pairs.append((eng, spa))\n",
        "\n",
        "import random\n",
        "print(random.choice(text_pairs))\n"
      ],
      "metadata": {
        "id": "8yXR9K8jg3P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shuffle and create the training, validation and test sets"
      ],
      "metadata": {
        "id": "rKWK-dr5hVmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "RziaVZtKheVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.26 Vectorizing the English and Spanish text pairs\n",
        "\n",
        "Note that for a non-toy translation model, we would treat punctuation characters as sep-\n",
        "arate tokens rather than stripping them, since we would want to be able to generate cor-\n",
        "rectly punctuated sentences. In our case, for simplicity, we’ll get rid of all punctuation."
      ],
      "metadata": {
        "id": "RSECS5X3hvn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "  return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "#To keep things simple, we’ll only look at\n",
        "#the top 15,000 words in each language,\n",
        "#and we’ll restrict sentences to 20 words.\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)\n"
      ],
      "metadata": {
        "id": "C6QrfjFTh3Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.27 Preparing datasets for the translation task\n",
        "\n",
        "Finally, we can turn our data into a tf.data pipeline. We want it to return a tuple\n",
        "(inputs, target) where inputs is a dict with two keys, “encoder_inputs” (the English\n",
        "sentence) and “decoder_inputs” (the Spanish sentence), and target is the Spanish\n",
        "sentence offset by one step ahead."
      ],
      "metadata": {
        "id": "nkcUNkPNjlQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "def format_dataset(eng, spa):\n",
        "  eng = source_vectorization(eng)\n",
        "  spa = target_vectorization(spa)\n",
        "  return ({\"english\": eng,\"spanish\": spa[:, :-1],}, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "  eng_texts, spa_texts = zip(*pairs)\n",
        "  eng_texts = list(eng_texts)\n",
        "  spa_texts = list(spa_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "#\n"
      ],
      "metadata": {
        "id": "vSLUZQy1jtkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset outputs\n"
      ],
      "metadata": {
        "id": "XChbmupokabq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "  print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")\n"
      ],
      "metadata": {
        "id": "RzXxS_evke-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.5.2 Sequence-to-sequence learning with RNNs"
      ],
      "metadata": {
        "id": "QY5r99kwoipp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.28 GRU-based encoder\n",
        "\n",
        "Let’s implement this in Keras with GRU-based encoders and decoders. The choice\n",
        "of GRU rather than LSTM makes things a bit simpler, since GRU only has a single\n",
        "state vector, whereas LSTM has multiple. Let’s start with the encoder.\n",
        "\n",
        "Q: Difference in GRU and LSTM layers"
      ],
      "metadata": {
        "id": "H5Qgn8cjk1kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(\n",
        "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ],
      "metadata": {
        "id": "X3s_9NhcoEth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.29 GRU-based decoder and the end-to-end model\n",
        "\n",
        "During training, the decoder takes as input the entire target sequence, but thanks to\n",
        "the step-by-step nature of RNNs, it only looks at tokens 0…N in the input to predict token N in the output (which corresponds to the next token in the sequence, since\n",
        "the output is intended to be offset by one step). This means we only use information\n",
        "from the past to predict the future, as we should; otherwise we’d be cheating, and our\n",
        "model would not work at inference time."
      ],
      "metadata": {
        "id": "DqjMNQZ1onJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "# End-to-end model: maps the source sentence and the target sentence to the target sentence one step in the future\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)\n"
      ],
      "metadata": {
        "id": "1NeMuxc2ouIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.30 Training our recurrent sequence-to-sequence model"
      ],
      "metadata": {
        "id": "5DuZdEgiqpCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "pstaVnBFk4B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.31 Translating new sentences with our RNN encoder and decoder"
      ],
      "metadata": {
        "id": "Pnkfg14_slZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Prepare a dict to convert token\n",
        "#index predictions to string tokens\n",
        "\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  decoded_sentence = \"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "    next_token_predictions = seq2seq_rnn.predict(\n",
        "        [tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "_xGo5GYosndq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(30):\n",
        "  input_sentence = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sentence)\n",
        "  print(decode_sequence(input_sentence))\n",
        "  print(\"-\"*50)"
      ],
      "metadata": {
        "id": "JzXoiolMt-sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.33 The TransformerDecoder"
      ],
      "metadata": {
        "id": "7F5KDEpevT0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = layers.MultiHeadAttention( num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.attention_2 = layers.MultiHeadAttention( num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),] )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "    self.layernorm_3 = layers.LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim,\n",
        "    })\n",
        "    return config\n"
      ],
      "metadata": {
        "id": "jcqDLf8bvXvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.34 TransformerDecoder method that generates a causal mask\n"
      ],
      "metadata": {
        "id": "PcKcfZcqwhCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate matrix of shape (sequence_length, sequence_length) with 1s in one half and 0s in the other.\n",
        "def get_causal_attention_mask(self, inputs):\n",
        "  input_shape = tf.shape(inputs)\n",
        "  batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "  i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "  j = tf.range(sequence_length)\n",
        "  mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "\n",
        "  #Replicate it along the batch axis to get a matrix of shape (batch_size, sequence_length, sequence_length)\n",
        "  mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "  mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "  return tf.tile(mask, mult)\n"
      ],
      "metadata": {
        "id": "scO2DuvSwu4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.35 The forward pass of the TransformerDecoder"
      ],
      "metadata": {
        "id": "UQUTpcrzxy96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call(self, inputs, encoder_outputs, mask=None):\n",
        "  causal_mask = self.get_causal_attention_mask(inputs)\n",
        "  if mask is not None:\n",
        "    padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "    padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "  attention_output_1 = self.attention_1(query=inputs, value=inputs, key=inputs, attention_mask=causal_mask)\n",
        "  attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "  attention_output_2 = self.attention_2(query=attention_output_1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask,)\n",
        "  attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
        "  proj_output = self.dense_proj(attention_output_2)\n",
        "  return self.layernorm_3(attention_output_2 + proj_output)\n",
        ""
      ],
      "metadata": {
        "id": "UgiV5iOPrHrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.36 End-to-end Transformer"
      ],
      "metadata": {
        "id": "0iBLV1zM08Nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "metadata": {
        "id": "PPsIntBw1A8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.37 Training the sequence-to-sequence Transformer"
      ],
      "metadata": {
        "id": "_WZBzNwk1tQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "hyaL6bjQ1vYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing 11.38 Translating new sentences with our Transformer model"
      ],
      "metadata": {
        "id": "EKX-he4B18D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  decoded_sentence = \"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "    predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence\n",
        ""
      ],
      "metadata": {
        "id": "7c1K7O8p2H5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "  input_sentence = random.choice(test_eng_texts)\n",
        "  print(\"-\")\n",
        "  print(input_sentence)\n",
        "  print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "id": "dNORRXbj232l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aoGt8NIsrHso"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}